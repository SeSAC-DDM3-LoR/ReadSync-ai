# ë¡œì»¬ë¡œ ëª¨ë¸ ì €ì¥
# from contextlib import asynccontextmanager
# from fastapi import FastAPI
# from sentence_transformers import SentenceTransformer
# import torch

# # ëª¨ë¸ì„ ë‹´ì•„ë‘˜ ê³µê°„
# ml_models = {}

# @asynccontextmanager
# async def lifespan(app: FastAPI):
#     # 1. [Startup] ëª¨ë¸ ë¡œë”©
#     model_id = "nlpai-lab/KURE-v1"
#     print(f"ğŸš€ {model_id} ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘...")

#     device = "cuda" if torch.cuda.is_available() else "cpu"
    
#     # SentenceTransformerë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
#     # KURE-v1ì€ Hugging Faceì—ì„œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
#     ml_models["embedding_model"] = SentenceTransformer(model_id, device=device)
    
#     print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì‹¤í–‰ ì¥ì¹˜: {device})")
    
#     yield
    
#     # 2. [Shutdown] ë©”ëª¨ë¦¬ ì •ë¦¬
#     ml_models.clear()
#     if torch.cuda.is_available():
#         torch.cuda.empty_cache()
#     print("ğŸ§¹ ì„œë²„ ì¢…ë£Œ: ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


# APIë¡œ í˜¸ì¶œ
# src/app/lifespan.py
from contextlib import asynccontextmanager
from fastapi import FastAPI
import logging

logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì¼œì§ˆ ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„
    logger.info("ğŸš€ API ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (Hugging Face Inference API ëª¨ë“œ)")
    
    yield
    
    # ì„œë²„ê°€ êº¼ì§ˆ ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„
    logger.info("ğŸ›‘ API ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤.")