# ëª¨ë¸ ë¡œë“œí•˜ëŠ” ê²½ìš°(hugging face apiê°€ ì•„ë‹Œ ë¡œì»¬ ë‹¤ìš´)
# from fastapi import APIRouter, HTTPException, status
# from pydantic import BaseModel, Field
# from app.lifespan import ml_models
# import logging

# # ë¡œê·¸ ê¸°ë¡ ì„¤ì •
# logger = logging.getLogger(__name__)
# router = APIRouter()

# # 1. ìë°”ì—ì„œ ë³´ë‚¼ ë°ì´í„° ê·œê²© (ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨)
# class EmbeddingRequest(BaseModel):
#     text: str = Field(..., min_length=1, description="ì„ë² ë”©í•  ë„ì„œì˜ ì œëª©ì´ë‚˜ ì¤„ê±°ë¦¬")

# # 2. ìë°”ì— ëŒë ¤ì¤„ ì‘ë‹µ ê·œê²©
# class EmbeddingResponse(BaseModel):
#     embedding: list[float] = Field(..., description="KURE-v1 ëª¨ë¸ì´ ìƒì„±í•œ 1024ì°¨ì› ë²¡í„°")

# @router.post(
#     "/embed", 
#     response_model=EmbeddingResponse,
#     status_code=status.HTTP_200_OK,
#     summary="ì‹¤ì‹œê°„ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"
# )
# async def get_embedding(request: EmbeddingRequest):
#     """
#     ìë°” ì„œë²„ë¡œë¶€í„° ë°›ì€ í…ìŠ¤íŠ¸ë¥¼ KURE-v1 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#     """
#     # [ì²´í¬] ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
#     if "embedding_model" not in ml_models:
#         logger.error("AI Model (KURE-v1) is not loaded in ml_models.")
#         raise HTTPException(
#             status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
#             detail="AI ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
#         )

#     try:
#         # [ì‹¤í–‰] ì„ë² ë”© ìƒì„±
#         model = ml_models["embedding_model"]
        
#         # SentenceTransformerì˜ encodeëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPU/GPU ìì›ì„ ì‚¬ìš©í•˜ë¯€ë¡œ 
#         # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.
#         # .tolist()ë¥¼ í˜¸ì¶œí•˜ì—¬ JSON ì‘ë‹µì´ ê°€ëŠ¥í•œ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#         vector = model.encode(request.text).tolist()
        
#         logger.info(f"Successfully generated embedding for text: {request.text[:20]}...")
#         return EmbeddingResponse(embedding=vector)

#     except Exception as e:
#         logger.error(f"Error during embedding generation: {str(e)}")
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=f"ì„ë² ë”© ìƒì„± ì¤‘ ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#         )

import os
import json
import numpy as np
import aioboto3
from fastapi import APIRouter, HTTPException
from typing import List
from urllib.parse import urlparse, unquote
from pydantic import BaseModel
from huggingface_hub import AsyncInferenceClient
from dotenv import load_dotenv


import re
import requests

# .env ë¡œë“œ ë° ì„¤ì •
load_dotenv()
router = APIRouter()
TOKEN_LIMIT = 3000 # ì²­í¬ ë¶„ì ˆ ê¸°ì¤€

HF_TOKEN = os.getenv("HF_TOKEN")
MODEL_ID = os.getenv("MODEL_ID")
client = AsyncInferenceClient(model=MODEL_ID, token=HF_TOKEN)

# 1. Google Drive ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
async def download_from_drive(google_drive_url: str):
    try:
        # íŒŒì¼ ID ì¶”ì¶œ
        file_id_match = re.search(r'/d/([^/]+)', google_drive_url)
        if not file_id_match:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
        
        file_id = file_id_match.group(1)
        
        # 2. ì§ì† ë‹¤ìš´ë¡œë“œ URL ìƒì„±
        download_url = f'https://drive.google.com/uc?export=download&id={file_id}'
        
        # 3. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = requests.get(download_url)
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail="êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        return response.json()
    except Exception as e:
        print(f"âŒ Drive ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

# 2. S3 ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
async def download_from_s3(s3_url: str):
    # ğŸ” 1. ë””ë²„ê¹…: í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ë“¤ì–´ì™”ëŠ”ì§€ ë¡œê·¸ë¡œ í™•ì¸
    ACCESS_KEY = os.getenv("AWS_ACCESS_KEY", "")
    SECRET_KEY = os.getenv("AWS_SECRET_KEY", "")
    REGION = os.getenv("AWS_REGION", "ap-northeast-2")
    # BUCKET_NAME = os.getenv("AWS_BUCKET_NAME", "")
    
    if not ACCESS_KEY or not SECRET_KEY:
        print("âŒ AWS ìê²© ì¦ëª…(í™˜ê²½ ë³€ìˆ˜)ì´ ì—†ìŠµë‹ˆë‹¤! docker-compose.ymlì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print(f"ğŸ”‘ AWS Key ë¡œë“œ ì„±ê³µ: {ACCESS_KEY[:4]}****")

    try:
        # ğŸ” 2. URL íŒŒì‹± ë¡œì§ (s3:// í”„ë¡œí† ì½œê³¼ https:// URL ëª¨ë‘ ëŒ€ì‘í•˜ë„ë¡ ë³´ì™„)
        parsed_url = urlparse(s3_url)
        
        # 's3://ë²„í‚·ëª…/í‚¤' í˜•ì‹ì¸ ê²½ìš°
        if parsed_url.scheme == 's3':
            bucket_name = parsed_url.netloc
            key = unquote(parsed_url.path.lstrip('/'))
        # 'https://ë²„í‚·ëª….s3...' í˜•ì‹ì¸ ê²½ìš°
        else:
            bucket_name = parsed_url.netloc.split('.')[0]
            key = unquote(parsed_url.path.lstrip('/'))

        # ğŸ” 3. ì„¸ì…˜ ìƒì„± ì‹œ ëª…ì‹œì ìœ¼ë¡œ ìê²© ì¦ëª… ì£¼ì… (ê°€ì¥ ì•ˆì „í•¨)
        session = aioboto3.Session(
            aws_access_key_id=ACCESS_KEY,
            aws_secret_access_key=SECRET_KEY,
            region_name=REGION
        )

        async with session.client('s3') as s3:
            print(f"â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {bucket_name}/{key}")
            response = await s3.get_object(Bucket=bucket_name, Key=key)
            async with response['Body'] as stream:
                file_content = await stream.read()
                return json.loads(file_content.decode('utf-8'))

    except Exception as e:
        print(f"âŒ S3 ë‹¤ìš´ë¡œë“œ ì—ëŸ¬ ìƒì„¸: {str(e)}")
        # ì—ëŸ¬ë¥¼ ê°ì¶”ì§€ ë§ê³  í˜¸ì¶œí•œ ìª½(FastAPI)ì—ì„œ 500 ì—ëŸ¬ ì›ì¸ì„ ì•Œ ìˆ˜ ìˆê²Œ ë˜ì§
        raise e

def aggregate_vectors(vectors, p=1.05, use_softmax=True):
    """
    ì—¬ëŸ¬ ë²¡í„°ë¥¼ ë¹„ì„ í˜• ê°€ì¤‘ì¹˜(Power Pooling + Softmax)ë¥¼ ì ìš©í•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.
    p: ì§€ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ 'ê°•í•œ íŠ¹ì§•'ì„ ê°€ì§„ ë²¡í„°ì˜ ì‹ í˜¸ê°€ ì¦í­ë©ë‹ˆë‹¤.
    """
    if not vectors:
        return None
    
    arr = np.array(vectors)
    
    # 1. ë²¡í„°ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (Softmax Weighting)
    if use_softmax:
        # ë²¡í„°ì˜ L2 Normì´ í°(ì •ë³´ëŸ‰ì´ ë§ì€) ë²¡í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        norms = np.linalg.norm(arr, axis=1)
        exp_norms = np.exp(norms - np.max(norms))
        weights = exp_norms / np.sum(exp_norms)
        # ê°€ì¤‘ì¹˜ ì ìš©
        arr = arr * weights[:, np.newaxis]
    
    # 2. ì›ì†Œë³„ ì‹ í˜¸ ì¦í­ (Power Pooling)
    # ë¶€í˜¸ ìœ ì§€í•˜ë©° pì œê³± ìˆ˜í–‰
    signed_power = np.sign(arr) * (np.abs(arr) ** p)
    
    # 3. í•©ì‚° ë° ì •ê·œí™”
    integrated_vec = np.sum(signed_power, axis=0)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ L2 ì •ê·œí™”
    norm = np.linalg.norm(integrated_vec)
    if norm > 1e-9:
        integrated_vec = integrated_vec / norm
        
    return integrated_vec
    
async def core_embedding_logic(path: str):
    if "drive.google.com" in path:
        book_data = await download_from_drive(path)
    elif "amazonaws.com" in path or ".s3." in path:
        book_data = await download_from_s3(path)
    else:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ ê²½ë¡œ í˜•ì‹ì…ë‹ˆë‹¤.")

    texts = [node['text'] for node in book_data.get('content', []) if 'text' in node]
        # chunks: List[str] = []
    embedding_list = [] # ì„ë² ë”©ëœ ë²¡í„°ê°’ë“¤ë§Œ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    current_chunk = ""
    
    for text in texts:
        if len(current_chunk) + len(text) > TOKEN_LIMIT:
            if current_chunk.strip():
                # [ì¦‰ì‹œ ì„ë² ë”©] ì§€ê¸ˆê¹Œì§€ ëª¨ì¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„° ì €ì¥
                vector = await client.feature_extraction(current_chunk.strip())
                embedding_list.append(vector)
                current_chunk = ""

            # 1-2. [ì¤‘ìš”] ìƒˆë¡œ ë“¤ì–´ì˜¨ text ìì²´ê°€ limitë³´ë‹¤ í¬ë‹¤ë©´? 
            # ì´ textë¥¼ limit ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ì¦‰ì‹œ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ì— ë„£ìŒ
            if len(text) > TOKEN_LIMIT:
                sub_chunks = [text[i : i + TOKEN_LIMIT] for i in range(0, len(text), TOKEN_LIMIT)]
                # ë§ˆì§€ë§‰ ì¡°ê°ì€ ë‹¤ìŒ textì™€ í•©ì¹˜ê¸° ìœ„í•´ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ì¦‰ì‹œ ì„ë² ë”©
                for sub in sub_chunks[:-1]:
                    vector = await client.feature_extraction(sub.strip())
                    embedding_list.append(vector)
                current_chunk = sub_chunks[-1] # ë§ˆì§€ë§‰ ì¡°ê°ë§Œ ìœ ì§€
            else:
                current_chunk = text

        else:
            current_chunk += " " + text
    
    if current_chunk.strip():
        vector = await client.feature_extraction(current_chunk.strip())
        embedding_list.append(vector)
        # chunks.append(current_chunk.strip())

    # if not chunks:
    if not embedding_list:
        raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # integrated_vector = np.mean(embedding_list, axis=0)
    integrated_vector = aggregate_vectors(embedding_list)

    result_list = integrated_vector.tolist() if hasattr(integrated_vector, "tolist") else integrated_vector

    print(f"ì´ ì„ë² ë”©ëœ ì²­í¬ ìˆ˜: {len(embedding_list)}")

    return result_list

class S3EmbeddingRequest(BaseModel):
    s3_url: str

class EmbeddingResponse(BaseModel):
    embedding: List[float]

@router.post("/embed-from-s3", response_model=EmbeddingResponse)
async def get_embedding(request: S3EmbeddingRequest):
    vector = await core_embedding_logic(request.s3_url)
    return {"embedding": vector}

    # except Exception as e:
    #     print(f"âŒ S3 ë¹„ë™ê¸° ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    #     raise HTTPException(status_code=500, detail=str(e))


class DriveEmbeddingRequest(BaseModel):
    google_drive_url: str

@router.post("/embed-from-drive")
async def get_embedding_from_drive(request: DriveEmbeddingRequest):
    vector = await core_embedding_logic(request.google_drive_url)
    return {"embedding": vector.tolist()}

    # except Exception as e:
    #     raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    

@router.post("/text-from-drive")
async def get_text_from_drive(request: DriveEmbeddingRequest) -> List[str]:
    # 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê³µìœ  ë§í¬ -> ì§ì† ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ë³€í™˜
    # ë§í¬ ì˜ˆì‹œ: https://drive.google.com/file/d/1A2B3C.../view?usp=sharing
    file_id_match = re.search(r'/d/([^/]+)', request.google_drive_url)
    if not file_id_match:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
    
    file_id = file_id_match.group(1)
    
    # 2. ì§ì† ë‹¤ìš´ë¡œë“œ URL ìƒì„±
    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'
    
    # 3. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    response = requests.get(download_url)
    if response.status_code != 200:
        raise HTTPException(status_code=500, detail="êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    book_data = response.json()


    # 3. ë°ì´í„° êµ¬ì¡°ì—ì„œ 'text' í•„ë“œë§Œ ì¶”ì¶œ (ì •ìš°ë‹˜ì˜ JSON ê·œê²© ê¸°ì¤€)
    # book_data['content'] ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒë©° 'text' í‚¤ê°€ ìˆëŠ” ê²ƒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    texts = [
        node['text'] 
        for node in book_data.get('content', []) 
        if 'text' in node
    ]

    return texts

class TextEmbeddingRequest(BaseModel):
    text: str

@router.post("/embed-text")
async def get_embedding_from_text(request: TextEmbeddingRequest):
    try:
        # í—ˆê¹…í˜ì´ìŠ¤ API í˜¸ì¶œ (await ì‚¬ìš©)
        embedding = await client.feature_extraction(request.text)
        
        # ë°˜í™˜ëœ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸ í›„ ì „ë‹¬
        # ë³´í†µ feature_extractionì€ ë¦¬ìŠ¤íŠ¸ë‚˜ ë„˜íŒŒì´ ë°°ì—´ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {"embedding": embedding.tolist() if hasattr(embedding, "tolist") else embedding}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HuggingFace API ì˜¤ë¥˜: {str(e)}")

class RagEmbeddingRequest(BaseModel):
    content: List[dict] # ì±…ì˜ content êµ¬ì¡° (text, id í¬í•¨)


class RagNode(BaseModel):
    text: str
    id: str | int | None  # id can be int or string, normalize to string later
    speaker: str | None = None

class RagEmbeddingRequest(BaseModel):
    content: List[RagNode]

class ChildChunk(BaseModel):
    content_text: str  # Renamed
    vector: List[float]
    chunk_index: int   # Added
    paragraph_ids: List[str] # Changed to list of strings

class ParentChunk(BaseModel):
    content_text: str # Renamed
    speaker_list: List[str]
    paragraph_ids: List[str] # Added
    start_paragraph_id: str # Changed to str
    end_paragraph_id: str   # Changed to str
    children: List[ChildChunk]

class EmbeddingRagResponse(BaseModel):
    parents: List[ParentChunk]

class EmbeddingQueryRequest(BaseModel):
    text: str

@router.post("/embed-query", response_model=EmbeddingResponse)
async def embed_query(request: EmbeddingQueryRequest):
    try:
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="Query text cannot be empty.")

        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
        vector = await client.feature_extraction(request.text.strip())
        
        # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
        result_list = vector.tolist() if hasattr(vector, "tolist") else vector
        return {"embedding": result_list}

    except Exception as e:
        print(f"âŒ Query ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/embed-rag-content", response_model=EmbeddingRagResponse)
async def embed_rag_content(request: RagEmbeddingRequest):
    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° êµ¬ì¡°í™” (Pydantic ëª¨ë¸ ì‚¬ìš©)
        content_nodes = request.content
        if not content_nodes:
            raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

        parents = []
        
        # Parent Chunking ì„¤ì •
        PARENT_CHUNK_SIZE = 20  # ë¬¸ë‹¨ ìˆ˜ ê¸°ì¤€
        PARENT_OVERLAP = 5      # 20% ì˜¤ë²„ë©
        
        # Child Chunking ì„¤ì •
        CHILD_CHUNK_SIZE = 5    # 5ë¬¸ë‹¨
        CHILD_OVERLAP = 1       # 1ë¬¸ë‹¨ ì˜¤ë²„ë©

        # ì „ì²´ ë…¸ë“œ ìˆœíšŒ
        parent_start_idx = 0
        
        while parent_start_idx < len(content_nodes):
            # 2-1. Parent Chunk ë²”ìœ„ ì„¤ì •
            parent_end_idx = min(parent_start_idx + PARENT_CHUNK_SIZE, len(content_nodes))
            
            # Parent Chunk ìƒì„±
            parent_nodes = content_nodes[parent_start_idx:parent_end_idx]
            
            # Parent ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            parent_text_builder = []
            parent_speakers = set()
            parent_para_ids = [] # Added for ID collection
            start_para_id = str(parent_nodes[0].id) if parent_nodes[0].id is not None else "0"
            end_para_id = str(parent_nodes[-1].id) if parent_nodes[-1].id is not None else "0"
            
            for node in parent_nodes:
                text_part = node.text
                if node.speaker:
                    text_part = f"{node.speaker}: {text_part}"
                    parent_speakers.add(node.speaker)
                parent_text_builder.append(text_part)
                parent_para_ids.append(str(node.id) if node.id is not None else "0") # Collect IDs
            
            parent_content = " ".join(parent_text_builder)
            
            # 2-2. Child Chunking (Parent ë‚´ë¶€ì—ì„œ ìˆ˜í–‰)
            children = []
            child_start_idx = 0 # Parent ë‚´ë¶€ ì¸ë±ìŠ¤
            
            while child_start_idx < len(parent_nodes):
                child_end_idx = min(child_start_idx + CHILD_CHUNK_SIZE, len(parent_nodes))
                child_nodes = parent_nodes[child_start_idx:child_end_idx]
                
                # Child ë©”íƒ€ë°ì´í„°
                child_text_builder = []
                child_para_ids = []

                for node in child_nodes:
                    text_part = node.text
                    if node.speaker:
                        text_part = f"{node.speaker}: {text_part}"
                    child_text_builder.append(text_part)
                    child_para_ids.append(str(node.id) if node.id is not None else "0")
                
                child_content = " ".join(child_text_builder)
                
                # Child Vector ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)
                if child_content.strip():
                     vector = await client.feature_extraction(child_content.strip())
                     
                     children.append(ChildChunk(
                         content_text=child_content,
                         vector=vector.tolist() if hasattr(vector, "tolist") else vector,
                         chunk_index=len(children), # í˜„ì¬ Parent ë‚´ì—ì„œì˜ ìˆœì„œ (0ë¶€í„° ì‹œì‘)
                         paragraph_ids=child_para_ids
                     ))
                
                # Child Loop Control
                if child_end_idx == len(parent_nodes):
                    break
                
                # ì¸ë±ìŠ¤ ì¦ê°€
                child_start_idx += (CHILD_CHUNK_SIZE - CHILD_OVERLAP)
            
            # Parent ê²°ê³¼ ì €ì¥
            parents.append(ParentChunk(
                content_text=parent_content,
                speaker_list=list(parent_speakers),
                paragraph_ids=parent_para_ids, # Added
                start_paragraph_id=start_para_id,
                end_paragraph_id=end_para_id,
                children=children
            ))

            # Parent Loop Control
            if parent_end_idx == len(content_nodes):
                break
                
            parent_start_idx += (PARENT_CHUNK_SIZE - PARENT_OVERLAP)

        print(f"âœ… RAG Parent ì²­í‚¹ ì™„ë£Œ: ì´ {len(parents)}ê°œ Parent ì²­í¬ ìƒì„±")
        return EmbeddingRagResponse(parents=parents)

    except Exception as e:
        print(f"âŒ RAG ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # traceback ì¶œë ¥ìœ¼ë¡œ ë””ë²„ê¹… ìš©ì´í•˜ê²Œ
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# --- [ìƒˆë¡œ ì¶”ê°€í•  ë°°ì¹˜ ì—”ë“œí¬ì¸íŠ¸] ---
@router.post("/embed-batch")
async def get_batch_embeddings(request: dict):
    paths = request.get("paths", [])
    chapter_vectors = []

    # ìë°”ê°€ ë³´ë‚¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì„ë² ë”©
    for path in paths:
        chapter_vec = await core_embedding_logic(path)
        if chapter_vec is not None:
            chapter_vectors.append(chapter_vec)

    if not chapter_vectors:
        raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë¶ ë²¡í„° ê³„ì‚° (ëª¨ë“  ì±•í„° ë²¡í„°ì˜ í‰ê· )
    # average_vector = np.mean(chapter_vectors, axis=0)
    average_vector = aggregate_vectors(chapter_vectors, 1)

    book_vector = average_vector.tolist() if hasattr(average_vector, "tolist") else average_vector

    return {
        "book_vector": book_vector,
        "chapter_vectors": [cv.tolist() if hasattr(cv, "tolist") else cv for cv in chapter_vectors]
    }