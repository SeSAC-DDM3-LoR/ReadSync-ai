# ëª¨ë¸ ë¡œë“œí•˜ëŠ” ê²½ìš°(hugging face apiê°€ ì•„ë‹Œ ë¡œì»¬ ë‹¤ìš´)
# from fastapi import APIRouter, HTTPException, status
# from pydantic import BaseModel, Field
# from app.lifespan import ml_models
# import logging

# # ë¡œê·¸ ê¸°ë¡ ì„¤ì •
# logger = logging.getLogger(__name__)
# router = APIRouter()

# # 1. ìë°”ì—ì„œ ë³´ë‚¼ ë°ì´í„° ê·œê²© (ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨)
# class EmbeddingRequest(BaseModel):
#     text: str = Field(..., min_length=1, description="ì„ë² ë”©í•  ë„ì„œì˜ ì œëª©ì´ë‚˜ ì¤„ê±°ë¦¬")

# # 2. ìë°”ì— ëŒë ¤ì¤„ ì‘ë‹µ ê·œê²©
# class EmbeddingResponse(BaseModel):
#     embedding: list[float] = Field(..., description="KURE-v1 ëª¨ë¸ì´ ìƒì„±í•œ 1024ì°¨ì› ë²¡í„°")

# @router.post(
#     "/embed", 
#     response_model=EmbeddingResponse,
#     status_code=status.HTTP_200_OK,
#     summary="ì‹¤ì‹œê°„ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"
# )
# async def get_embedding(request: EmbeddingRequest):
#     """
#     ìë°” ì„œë²„ë¡œë¶€í„° ë°›ì€ í…ìŠ¤íŠ¸ë¥¼ KURE-v1 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#     """
#     # [ì²´í¬] ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
#     if "embedding_model" not in ml_models:
#         logger.error("AI Model (KURE-v1) is not loaded in ml_models.")
#         raise HTTPException(
#             status_code=status.HTTP_503_SERVICE_UNAVAILABLE, 
#             detail="AI ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
#         )

#     try:
#         # [ì‹¤í–‰] ì„ë² ë”© ìƒì„±
#         model = ml_models["embedding_model"]
        
#         # SentenceTransformerì˜ encodeëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPU/GPU ìì›ì„ ì‚¬ìš©í•˜ë¯€ë¡œ 
#         # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.
#         # .tolist()ë¥¼ í˜¸ì¶œí•˜ì—¬ JSON ì‘ë‹µì´ ê°€ëŠ¥í•œ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#         vector = model.encode(request.text).tolist()
        
#         logger.info(f"Successfully generated embedding for text: {request.text[:20]}...")
#         return EmbeddingResponse(embedding=vector)

#     except Exception as e:
#         logger.error(f"Error during embedding generation: {str(e)}")
#         raise HTTPException(
#             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
#             detail=f"ì„ë² ë”© ìƒì„± ì¤‘ ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#         )

import os
import json
import numpy as np
import aioboto3
import asyncio
from fastapi import APIRouter, HTTPException
from typing import List
from urllib.parse import urlparse, unquote
from pydantic import BaseModel
from huggingface_hub import AsyncInferenceClient
from dotenv import load_dotenv


import re
import requests

# .env ë¡œë“œ ë° ì„¤ì •
load_dotenv()
router = APIRouter()
TOKEN_LIMIT = 2000 # ì²­í¬ ë¶„ì ˆ ê¸°ì¤€

HF_TOKEN = os.getenv("HF_TOKEN")
MODEL_ID = os.getenv("MODEL_ID")
client = AsyncInferenceClient(model=MODEL_ID, token=HF_TOKEN)

# [Global Limiter] ì „ì²´ ì• í”Œë¦¬ì¼€ì´ì…˜ ìˆ˜ì¤€ì—ì„œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
# í•¨ìˆ˜ ë‚´ë¶€ê°€ ì•„ë‹Œ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•´ì•¼ ì—¬ëŸ¬ ì±…ì„ ë™ì‹œì— ì²˜ë¦¬í•  ë•Œë„ ì´í•©ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
GLOBAL_SEMAPHORE = asyncio.Semaphore(20) # 5 -> 20ìœ¼ë¡œ ìƒí–¥ (ì•ˆì •ì„± í™•ì¸ í•„ìš”)

# 1. Google Drive ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
async def download_from_drive(google_drive_url: str):
    try:
        # íŒŒì¼ ID ì¶”ì¶œ
        file_id_match = re.search(r'/d/([^/]+)', google_drive_url)
        if not file_id_match:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
        
        file_id = file_id_match.group(1)
        
        # 2. ì§ì† ë‹¤ìš´ë¡œë“œ URL ìƒì„±
        download_url = f'https://drive.google.com/uc?export=download&id={file_id}'
        
        # 3. íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Non-blocking)
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, requests.get, download_url)
        
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail="êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        return response.json()
    except Exception as e:
        print(f"âŒ Drive ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

# 2. S3 ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
async def download_from_s3(s3_url: str):
    # ğŸ” 1. ë””ë²„ê¹…: í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ë“¤ì–´ì™”ëŠ”ì§€ ë¡œê·¸ë¡œ í™•ì¸
    ACCESS_KEY = os.getenv("AWS_ACCESS_KEY", "")
    SECRET_KEY = os.getenv("AWS_SECRET_KEY", "")
    REGION = os.getenv("AWS_REGION", "ap-northeast-2")
    # BUCKET_NAME = os.getenv("AWS_BUCKET_NAME", "")
    
    if not ACCESS_KEY or not SECRET_KEY:
        print("âŒ AWS ìê²© ì¦ëª…(í™˜ê²½ ë³€ìˆ˜)ì´ ì—†ìŠµë‹ˆë‹¤! docker-compose.ymlì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print(f"ğŸ”‘ AWS Key ë¡œë“œ ì„±ê³µ: {ACCESS_KEY[:4]}****")

    try:
        # ğŸ” 2. URL íŒŒì‹± ë¡œì§ (s3:// í”„ë¡œí† ì½œê³¼ https:// URL ëª¨ë‘ ëŒ€ì‘í•˜ë„ë¡ ë³´ì™„)
        parsed_url = urlparse(s3_url)
        
        # 's3://ë²„í‚·ëª…/í‚¤' í˜•ì‹ì¸ ê²½ìš°
        if parsed_url.scheme == 's3':
            bucket_name = parsed_url.netloc
            key = unquote(parsed_url.path.lstrip('/'))
        # 'https://ë²„í‚·ëª….s3...' í˜•ì‹ì¸ ê²½ìš°
        else:
            bucket_name = parsed_url.netloc.split('.')[0]
            key = unquote(parsed_url.path.lstrip('/'))

        # ğŸ” 3. ì„¸ì…˜ ìƒì„± ì‹œ ëª…ì‹œì ìœ¼ë¡œ ìê²© ì¦ëª… ì£¼ì… (ê°€ì¥ ì•ˆì „í•¨)
        session = aioboto3.Session(
            aws_access_key_id=ACCESS_KEY,
            aws_secret_access_key=SECRET_KEY,
            region_name=REGION
        )

        async with session.client('s3') as s3:
            print(f"â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {bucket_name}/{key}")
            response = await s3.get_object(Bucket=bucket_name, Key=key)
            async with response['Body'] as stream:
                file_content = await stream.read()
                return json.loads(file_content.decode('utf-8'))

    except Exception as e:
        print(f"âŒ S3 ë‹¤ìš´ë¡œë“œ ì—ëŸ¬ ìƒì„¸: {str(e)}")
        # ì—ëŸ¬ë¥¼ ê°ì¶”ì§€ ë§ê³  í˜¸ì¶œí•œ ìª½(FastAPI)ì—ì„œ 500 ì—ëŸ¬ ì›ì¸ì„ ì•Œ ìˆ˜ ìˆê²Œ ë˜ì§
        raise e

def aggregate_vectors(vectors):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²­í¬ì˜ ë²¡í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    [Hybrid Aggregation] Mean Pooling + Max Pooling
    """
    if not vectors:
        return None
    
    
    arr = np.array(vectors)
    
    # [Main] Power Mean Aggregation (p=3)
    # ì‚°ìˆ  í‰ê· (p=1)ê³¼ Max Pooling(p=ë¬´í•œëŒ€)ì˜ ì ˆì¶©ì•ˆ.
    # ê° ì°¨ì›ë³„ ê°’ì˜ í¬ê¸°ë¥¼ pì œê³±í•˜ì—¬ í‰ê· ì„ ëƒ„ìœ¼ë¡œì¨,
    # ê°•í•˜ê²Œ ë°œí˜„ëœ íŠ¹ì§•(Keyword/Theme)ì„ "ì ë‹¹íˆ ê°•ì¡°"í•˜ê³ (Sharpening), 
    # ë„ˆë¬´ ì•½í•œ ì‹ í˜¸(Noise)ëŠ” ì–µì œí•©ë‹ˆë‹¤.
    try:
        # 0. ì‚¬ì „ ì •ê·œí™” (í•„ìˆ˜)
        # Power Meanì„ ì“°ë ¤ë©´ ê° ë²¡í„°ì˜ ìŠ¤ì¼€ì¼ì´ ë§ì¶°ì ¸ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        norms = np.linalg.norm(arr, axis=1, keepdims=True)
        arr_normalized = arr / (norms + 1e-9)

        # 1. Power Mean (p=2.5)
        # p=3ì´ ë„ˆë¬´ íŠˆ ìˆ˜ ìˆë‹¤ëŠ” ìš°ë ¤ë¥¼ ë°˜ì˜í•˜ì—¬ 2.5ë¡œ í•˜í–¥ ì¡°ì •.
        # ì—¬ì „íˆ íŠ¹ì§•ì€ ì˜ ì‚´ë¦¬ì§€ë§Œ(Sharpening), 3.0ë³´ë‹¤ëŠ” ë¶€ë“œëŸ½ê³  ì•ˆì •ì ì…ë‹ˆë‹¤.
        p = 2.5
        power_arr = np.sign(arr_normalized) * np.power(np.abs(arr_normalized), p)
        mean_vec = np.mean(power_arr, axis=0)
        
        # 2. ë‹¤ì‹œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì› (Inverse Power)
        # integrated_vec = sign(mean) * |mean|^(1/p)
        integrated_vec = np.sign(mean_vec) * np.power(np.abs(mean_vec), 1.0/p)
        
    except Exception as e:
        print(f"âš ï¸ Aggregation Failed, using simple mean: {e}")
        integrated_vec = np.mean(arr, axis=0)

    # ìµœì¢… ì •ê·œí™” (L2 Norm)
    norm = np.linalg.norm(integrated_vec)
    if norm > 1e-9:
        integrated_vec = integrated_vec / norm
        
    return integrated_vec
    
async def core_embedding_logic(path: str):
    if "drive.google.com" in path:
        book_data = await download_from_drive(path)
    elif "amazonaws.com" in path or ".s3." in path:
        book_data = await download_from_s3(path)
    else:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ ê²½ë¡œ í˜•ì‹ì…ë‹ˆë‹¤.")

    texts = [node['text'] for node in book_data.get('content', []) if 'text' in node]
    
    # 1. ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ëª¨ë‘ ë¶„í•  (ë©”ëª¨ë¦¬ ì‘ì—…)
    chunks_to_embed = []
    current_chunk = ""
    
    for text in texts:
        if len(current_chunk) + len(text) > TOKEN_LIMIT:
            if current_chunk.strip():
                chunks_to_embed.append(current_chunk.strip())
                current_chunk = ""

            # 1-2. [ì¤‘ìš”] ìƒˆë¡œ ë“¤ì–´ì˜¨ text ìì²´ê°€ limitë³´ë‹¤ í¬ë‹¤ë©´? 
            if len(text) > TOKEN_LIMIT:
                sub_chunks = [text[i : i + TOKEN_LIMIT] for i in range(0, len(text), TOKEN_LIMIT)]
                # ë§ˆì§€ë§‰ ì¡°ê°ì€ ë‹¤ìŒ textì™€ í•©ì¹˜ê¸° ìœ„í•´ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ì¦‰ì‹œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                for sub in sub_chunks[:-1]:
                    if sub.strip():
                        chunks_to_embed.append(sub.strip())
                current_chunk = sub_chunks[-1] # ë§ˆì§€ë§‰ ì¡°ê°ë§Œ ìœ ì§€
            else:
                current_chunk = text

        else:
            current_chunk += " " + text
    
    # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
    if current_chunk.strip():
        chunks_to_embed.append(current_chunk.strip())

    if not chunks_to_embed:
        raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # 2. [ë³‘ë ¬ ì²˜ë¦¬] ëª¨ì•„ë‘” ì²­í¬ë¥¼ í•œêº¼ë²ˆì— ì„ë² ë”© ìš”ì²­
    # ê¸°ì¡´: asyncio.gatherë¡œ ë¬´ì œí•œ ìš”ì²­ -> 504 Gateway Timeout ë°œìƒ
    # ë³€ê²½: Global Semaphoreë¡œ ì „ì²´ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ + Retry ë¡œì§ ì ìš©

    async def safe_embedding_request(text_chunk):
        async with GLOBAL_SEMAPHORE:
            max_retries = 5 # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
            base_delay = 0.5 # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶• (2s -> 0.5s)
            
            for attempt in range(max_retries):
                try:
                    return await client.feature_extraction(text_chunk)
                except Exception as e:
                    # 504(Gateway Timeout), 502(Bad Gateway), 429(Too Many Requests) ë“±ì€ ì¬ì‹œë„ ê°€ì¹˜ ìˆìŒ
                    error_msg = str(e)
                    if "504" in error_msg or "502" in error_msg or "429" in error_msg:
                        if attempt < max_retries - 1:
                            # Exponential Backoff with Jitter (Optional)
                            wait_time = base_delay * (2 ** attempt) 
                            # print(f"âš ï¸ API Error ({e}), retrying in {wait_time}s... (Attempt {attempt + 1}/{max_retries})")
                            await asyncio.sleep(wait_time)
                            continue
                    
                    # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬ì´ê±°ë‚˜ íšŸìˆ˜ ì´ˆê³¼ ì‹œ
                    print(f"âŒ Feature Extraction Failed after {attempt+1} attempts: {e}")
                    raise e

    try:
        tasks = [safe_embedding_request(chunk) for chunk in chunks_to_embed]
        embedding_list = await asyncio.gather(*tasks)
    except Exception as e:
        print(f"âŒ Parallel Embedding Error: {e}")
        raise e

    # if not chunks:
    if not embedding_list:
        raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    # integrated_vector = np.mean(embedding_list, axis=0)
    integrated_vector = aggregate_vectors(embedding_list)

    result_list = integrated_vector.tolist() if hasattr(integrated_vector, "tolist") else integrated_vector

    print(f"ì´ ì„ë² ë”©ëœ ì²­í¬ ìˆ˜: {len(embedding_list)}")

    return result_list

class S3EmbeddingRequest(BaseModel):
    s3_url: str

class EmbeddingResponse(BaseModel):
    embedding: List[float]

@router.post("/embed-from-s3", response_model=EmbeddingResponse)
async def get_embedding(request: S3EmbeddingRequest):
    vector = await core_embedding_logic(request.s3_url)
    return {"embedding": vector}

    # except Exception as e:
    #     print(f"âŒ S3 ë¹„ë™ê¸° ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    #     raise HTTPException(status_code=500, detail=str(e))


class DriveEmbeddingRequest(BaseModel):
    google_drive_url: str

@router.post("/embed-from-drive")
async def get_embedding_from_drive(request: DriveEmbeddingRequest):
    vector = await core_embedding_logic(request.google_drive_url)
    return {"embedding": vector.tolist()}

    # except Exception as e:
    #     raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    

@router.post("/text-from-drive")
async def get_text_from_drive(request: DriveEmbeddingRequest) -> List[str]:
    # 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê³µìœ  ë§í¬ -> ì§ì† ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ë³€í™˜
    # ë§í¬ ì˜ˆì‹œ: https://drive.google.com/file/d/1A2B3C.../view?usp=sharing
    file_id_match = re.search(r'/d/([^/]+)', request.google_drive_url)
    if not file_id_match:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
    
    file_id = file_id_match.group(1)
    
    # 2. ì§ì† ë‹¤ìš´ë¡œë“œ URL ìƒì„±
    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'
    
    # 3. íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Non-blocking)
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(None, requests.get, download_url)

    if response.status_code != 200:
        raise HTTPException(status_code=500, detail="êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    book_data = response.json()


    # 3. ë°ì´í„° êµ¬ì¡°ì—ì„œ 'text' í•„ë“œë§Œ ì¶”ì¶œ (ì •ìš°ë‹˜ì˜ JSON ê·œê²© ê¸°ì¤€)
    # book_data['content'] ë¦¬ìŠ¤íŠ¸ë¥¼ ëŒë©° 'text' í‚¤ê°€ ìˆëŠ” ê²ƒë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    texts = [
        node['text'] 
        for node in book_data.get('content', []) 
        if 'text' in node
    ]

    return texts

class TextEmbeddingRequest(BaseModel):
    text: str

@router.post("/embed-text")
async def get_embedding_from_text(request: TextEmbeddingRequest):
    try:
        # í—ˆê¹…í˜ì´ìŠ¤ API í˜¸ì¶œ (await ì‚¬ìš©)
        embedding = await client.feature_extraction(request.text)
        
        # ë°˜í™˜ëœ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸ í›„ ì „ë‹¬
        # ë³´í†µ feature_extractionì€ ë¦¬ìŠ¤íŠ¸ë‚˜ ë„˜íŒŒì´ ë°°ì—´ í˜•íƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {"embedding": embedding.tolist() if hasattr(embedding, "tolist") else embedding}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HuggingFace API ì˜¤ë¥˜: {str(e)}")

class RagEmbeddingRequest(BaseModel):
    content: List[dict] # ì±…ì˜ content êµ¬ì¡° (text, id í¬í•¨)


class RagNode(BaseModel):
    text: str
    id: str | int | None  # id can be int or string, normalize to string later
    speaker: str | None = None

class RagEmbeddingRequest(BaseModel):
    content: List[RagNode]

class ChildChunk(BaseModel):
    content_text: str  # Renamed
    vector: List[float]
    chunk_index: int   # Added
    paragraph_ids: List[str] # Changed to list of strings

class ParentChunk(BaseModel):
    content_text: str # Renamed
    speaker_list: List[str]
    paragraph_ids: List[str] # Added
    start_paragraph_id: str # Changed to str
    end_paragraph_id: str   # Changed to str
    children: List[ChildChunk]

class EmbeddingRagResponse(BaseModel):
    parents: List[ParentChunk]

class EmbeddingQueryRequest(BaseModel):
    text: str

@router.post("/embed-query", response_model=EmbeddingResponse)
async def embed_query(request: EmbeddingQueryRequest):
    try:
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="Query text cannot be empty.")

        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
        vector = await client.feature_extraction(request.text.strip())
        
        # ì•ˆì „í•œ íƒ€ì… ë³€í™˜
        result_list = vector.tolist() if hasattr(vector, "tolist") else vector
        return {"embedding": result_list}

    except Exception as e:
        print(f"âŒ Query ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/embed-rag-content", response_model=EmbeddingRagResponse)
async def embed_rag_content(request: RagEmbeddingRequest):
    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° êµ¬ì¡°í™” (Pydantic ëª¨ë¸ ì‚¬ìš©)
        content_nodes = request.content
        if not content_nodes:
            raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

        parents = []
        
        # Parent Chunking ì„¤ì •
        PARENT_CHUNK_SIZE = 20  # ë¬¸ë‹¨ ìˆ˜ ê¸°ì¤€
        PARENT_OVERLAP = 5      # 20% ì˜¤ë²„ë©
        
        # Child Chunking ì„¤ì •
        CHILD_CHUNK_SIZE = 5    # 5ë¬¸ë‹¨
        CHILD_OVERLAP = 1       # 1ë¬¸ë‹¨ ì˜¤ë²„ë©

        # ì „ì²´ ë…¸ë“œ ìˆœíšŒ
        parent_start_idx = 0
        
        while parent_start_idx < len(content_nodes):
            # 2-1. Parent Chunk ë²”ìœ„ ì„¤ì •
            parent_end_idx = min(parent_start_idx + PARENT_CHUNK_SIZE, len(content_nodes))
            
            # Parent Chunk ìƒì„±
            parent_nodes = content_nodes[parent_start_idx:parent_end_idx]
            
            # Parent ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            parent_text_builder = []
            parent_speakers = set()
            parent_para_ids = [] # Added for ID collection
            start_para_id = str(parent_nodes[0].id) if parent_nodes[0].id is not None else "0"
            end_para_id = str(parent_nodes[-1].id) if parent_nodes[-1].id is not None else "0"
            
            for node in parent_nodes:
                text_part = node.text
                if node.speaker:
                    text_part = f"{node.speaker}: {text_part}"
                    parent_speakers.add(node.speaker)
                parent_text_builder.append(text_part)
                parent_para_ids.append(str(node.id) if node.id is not None else "0") # Collect IDs
            
            parent_content = " ".join(parent_text_builder)
            
            # 2-2. Child Chunking (Parent ë‚´ë¶€ì—ì„œ ìˆ˜í–‰)
            children = []
            child_start_idx = 0 # Parent ë‚´ë¶€ ì¸ë±ìŠ¤
            
            while child_start_idx < len(parent_nodes):
                child_end_idx = min(child_start_idx + CHILD_CHUNK_SIZE, len(parent_nodes))
                child_nodes = parent_nodes[child_start_idx:child_end_idx]
                
                # Child ë©”íƒ€ë°ì´í„°
                child_text_builder = []
                child_para_ids = []

                for node in child_nodes:
                    text_part = node.text
                    if node.speaker:
                        text_part = f"{node.speaker}: {text_part}"
                    child_text_builder.append(text_part)
                    child_para_ids.append(str(node.id) if node.id is not None else "0")
                
                child_content = " ".join(child_text_builder)
                
                # Child Vector ìƒì„± (ë¹„ë™ê¸° ì²˜ë¦¬)
                if child_content.strip():
                     vector = await client.feature_extraction(child_content.strip())
                     
                     children.append(ChildChunk(
                         content_text=child_content,
                         vector=vector.tolist() if hasattr(vector, "tolist") else vector,
                         chunk_index=len(children), # í˜„ì¬ Parent ë‚´ì—ì„œì˜ ìˆœì„œ (0ë¶€í„° ì‹œì‘)
                         paragraph_ids=child_para_ids
                     ))
                
                # Child Loop Control
                if child_end_idx == len(parent_nodes):
                    break
                
                # ì¸ë±ìŠ¤ ì¦ê°€
                child_start_idx += (CHILD_CHUNK_SIZE - CHILD_OVERLAP)
            
            # Parent ê²°ê³¼ ì €ì¥
            parents.append(ParentChunk(
                content_text=parent_content,
                speaker_list=list(parent_speakers),
                paragraph_ids=parent_para_ids, # Added
                start_paragraph_id=start_para_id,
                end_paragraph_id=end_para_id,
                children=children
            ))

            # Parent Loop Control
            if parent_end_idx == len(content_nodes):
                break
                
            parent_start_idx += (PARENT_CHUNK_SIZE - PARENT_OVERLAP)

        print(f"âœ… RAG Parent ì²­í‚¹ ì™„ë£Œ: ì´ {len(parents)}ê°œ Parent ì²­í¬ ìƒì„±")
        return EmbeddingRagResponse(parents=parents)

    except Exception as e:
        print(f"âŒ RAG ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # traceback ì¶œë ¥ìœ¼ë¡œ ë””ë²„ê¹… ìš©ì´í•˜ê²Œ
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# --- [ìƒˆë¡œ ì¶”ê°€í•  ë°°ì¹˜ ì—”ë“œí¬ì¸íŠ¸] ---
import asyncio

# --- [ìƒˆë¡œ ì¶”ê°€í•  ë°°ì¹˜ ì—”ë“œí¬ì¸íŠ¸] ---
@router.post("/embed-batch")
async def get_batch_embeddings(request: dict):
    paths = request.get("paths", [])
    chapter_vectors = []

    # [ìˆ˜ì •] asyncio.gatherë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
    # ê¸°ì¡´: ìˆœì°¨ì  await -> ëŠë¦¼
    # ë³€ê²½: ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ ì²˜ë¦¬ -> ë¹ ë¦„
    
    # 1. íƒœìŠ¤í¬ ìƒì„±
    tasks = [core_embedding_logic(path) for path in paths]
    
    # 2. ë³‘ë ¬ ì‹¤í–‰
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 3. ê²°ê³¼ ìˆ˜ì§‘ (ì—ëŸ¬ê°€ ë‚œ í•­ëª©ì€ ì œì™¸í•˜ê±°ë‚˜ ë¡œê¹…)
    for res in results:
        if isinstance(res, Exception):
            print(f"âš ï¸ Batch processing error: {res}")
        elif res is not None:
            chapter_vectors.append(res)

    if not chapter_vectors:
        raise HTTPException(status_code=400, detail="ì„ë² ë”©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë¶ ë²¡í„° ê³„ì‚° (ëª¨ë“  ì±•í„° ë²¡í„°ì˜ í‰ê· )
    # average_vector = np.mean(chapter_vectors, axis=0)
    average_vector = aggregate_vectors(chapter_vectors)

    book_vector = average_vector.tolist() if hasattr(average_vector, "tolist") else average_vector

    return {
        "book_vector": book_vector,
        "chapter_vectors": [cv.tolist() if hasattr(cv, "tolist") else cv for cv in chapter_vectors]
    }